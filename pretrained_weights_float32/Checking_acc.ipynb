{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOW device is using: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn           #  Feedforward and Loss function\n",
    "import torch.nn.functional as F #  Functions that dont accept parameters \n",
    "from torch.utils.data import DataLoader  #  Data set Management i.e. creat mini-Batches\n",
    "\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "\n",
    "# Hyperparameters\n",
    "in_channels = 2 \n",
    "num_classes = 11 \n",
    "learning_rate = 1e-5\n",
    "batch_size = 8\n",
    "########## SNN ###########\n",
    "T_BIN = 15\n",
    "VTH = 0.3       #0.3\n",
    "DECAY = 0.3     #0.3\n",
    "########## Surrogate ###########\n",
    "alpha = 0.5  #alpha = lens*2\n",
    "\n",
    "#Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"NOW device is using: {device}\")\n",
    "\n",
    "\n",
    "# Dataset - DVS-Gesture\n",
    "# 2 x 128 x 128\n",
    "sensor_size = tonic.datasets.DVSGesture.sensor_size\n",
    "frame_transform = transforms.Compose([transforms.Denoise(filter_time=10000),\n",
    "                                      transforms.ToFrame(sensor_size=sensor_size, n_time_bins=T_BIN)])\n",
    "    \n",
    "trainset = tonic.datasets.DVSGesture(save_to=\"../../Torch_condaENV/Working_folder/dataset/\",transform=frame_transform, train=True)\n",
    "testset = tonic.datasets.DVSGesture(save_to=\"../../Torch_condaENV/Working_folder/dataset/\", transform=frame_transform, train=False)\n",
    "\n",
    "#collation整理 => pad out填充 shorting recordings to have same dimension\n",
    "train_loader = DataLoader(\n",
    "    dataset = trainset,\n",
    "    batch_size= batch_size,\n",
    "    collate_fn= tonic.collation.PadTensors(batch_first=False),\n",
    "    shuffle = True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = testset,\n",
    "    batch_size= batch_size,\n",
    "    collate_fn= tonic.collation.PadTensors(batch_first=False),\n",
    "    shuffle = False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "class CSNN_Model(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,num_classes):\n",
    "        super(CSNN_Model,self).__init__()\n",
    "\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv0 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)  #128x128 \n",
    "        self.conv1 = nn.Conv2d( 64 , 128, kernel_size=3,  stride=1, padding=1, bias=False)        #64x64 \n",
    "        self.conv2 = nn.Conv2d( 128 , 128, kernel_size=3, stride=1, padding=1, bias=False)       #32x32\n",
    "        self.conv3 = nn.Conv2d( 128 , 256, kernel_size=3, stride=2, padding=1, bias=False)       #16x16\n",
    "\n",
    "        self.fc1   = nn.Linear(4 * 4 * 256, 1024, bias = False)  # 4096*1024 \n",
    "        self.fc2   = nn.Linear(1024, num_classes, bias = False) \n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        # Reseting Neurons\n",
    "        c0_mem = c0_spike = torch.zeros(batch_size, 64, 128, 128,  device=device)\n",
    "        c1_mem = c1_spike = torch.zeros(batch_size, 128, 64, 64, device=device) \n",
    "        c2_mem = c2_spike = torch.zeros(batch_size, 128, 32, 32, device=device)\n",
    "        c3_mem = c3_spike = torch.zeros(batch_size, 256, 8, 8, device=device)\n",
    "\n",
    "        h1_mem = h1_spike = torch.zeros(batch_size, 1024, device=device)\n",
    "        h2_mem = h2_spike = h2_sumspike = torch.zeros(batch_size, num_classes, device=device)\n",
    "\n",
    "        for i in range(T_BIN): # Every single piece of t belongs to T\n",
    "            # .view change shape/dtype of tensor #####  -1 squeeze all dimension to 1 \n",
    "            # #1 tensor(N,P,H,W,T) to vector, #2 vector to tensor(N,-1)\n",
    "            x = input[i,:,:,:,:].to(device)\n",
    "\n",
    "            c0_mem, c0_spike = mem_update(self.conv0, x, c0_mem, c0_spike)\n",
    "            p0_spike = self.pool(c0_spike)\n",
    "\n",
    "            c1_mem, c1_spike = mem_update(self.conv1, p0_spike, c1_mem, c1_spike) \n",
    "            p1_spike = self.pool(c1_spike) \n",
    "\n",
    "            c2_mem, c2_spike = mem_update(self.conv2, p1_spike, c2_mem, c2_spike) \n",
    "            p2_spike = self.pool(c2_spike) \n",
    "\n",
    "            c3_mem, c3_spike = mem_update(self.conv3, p2_spike, c3_mem, c3_spike) \n",
    "            # print(torch.logical_or(p2_spike, p2_spike).to(torch.float32))\n",
    "            p3_spike = self.pool(c3_spike) \n",
    "\n",
    "            x = p3_spike.view(batch_size, -1)\n",
    "\n",
    "            h1_mem, h1_spike = mem_update(self.fc1, x, h1_mem, h1_spike)\n",
    "            h2_mem, h2_spike = mem_update(self.fc2, h1_spike, h2_mem, h2_spike)\n",
    "            h2_sumspike += h2_spike\n",
    "\n",
    "        # Where SumSpike = (N,#neurons) (2D matrix)/scalar \n",
    "        outputs = h2_sumspike / T_BIN\n",
    "        # print(torch.mean(outputs,dim=0))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ActivationFun(torch.autograd.Function):\n",
    "# For forward: 1/0 spike\n",
    "# For backward: Surrogate gradient -> unit retangular functionv rect(t) = 1/a if -a/2 < t < +a/2\n",
    "# h1(t) in spatio-temporal backpropagation by (Wu etal., 2018) \n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.gt(VTH).float()      # torch.gt(a,b) compare a and b : return 1/0 spike\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        temp = abs(input - VTH) < alpha/2    # lens = alpha/2\n",
    "        return grad_input * temp.float() / alpha  # intensify spiking output (Wu et. 2018 w/o 2*len) \n",
    "\n",
    "act_fun = ActivationFun.apply\n",
    "\n",
    "def mem_update(fc, x, volt, spike):\n",
    "    volt = volt * DECAY * (1 - spike) + fc(x)\n",
    "    spike = act_fun(volt)\n",
    "    return volt, spike\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network initialization \n",
    "model = CSNN_Model(in_channels=in_channels,num_classes=num_classes).to(device)\n",
    "\n",
    "# weight_path= \"./pretrained_DVS_csnn_128e_91a.t7\"\n",
    "weight_path = \"./pretrained_DVS_csnn_128e_91a.t7\"\n",
    "checkpoint = torch.load(weight_path,map_location=device)\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#Model evaluation\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking on training data\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Checking on testing data\")\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_sample = 0\n",
    "    model.eval()  #(Equivalent to model.train(False)) Nothing learn\n",
    "\n",
    "    with torch.no_grad():   #no need to cal grad\n",
    "        for image,label in loader:\n",
    "            image= image.to(device)\n",
    "            label= label.to(device)\n",
    "            \n",
    "            # T x N x 2312 => N x 2312\n",
    "            out_firing = model(image)\n",
    "\n",
    "            #64x10 output\n",
    "            _ , prediction = out_firing.max(1)  #64x1 (value in 2nd dimension)\n",
    "            num_correct += (prediction==label).sum()\n",
    "            num_sample += prediction.size(0)  #64 (value in 1st dimension)\n",
    "            \n",
    "        print(f'Got {num_correct}/{num_sample} with accuracy {float(num_correct)/float(num_sample)*100:.2f}')\n",
    "    \n",
    "    model.train() #Set back to train mode\n",
    "    return num_correct/num_sample    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = checkpoint['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv0.weight', 'conv1.weight', 'conv2.weight', 'conv3.weight', 'fc1.weight', 'fc2.weight']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layer_names = list(a.keys())\n",
    "print(layer_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2, 3, 3])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([11, 1024])\n",
      "[torch.Size([64, 2, 3, 3]), torch.Size([128, 64, 3, 3]), torch.Size([128, 128, 3, 3]), torch.Size([256, 128, 3, 3]), torch.Size([1024, 4096]), torch.Size([11, 1024])]\n"
     ]
    }
   ],
   "source": [
    "i = []\n",
    "for weight in model.parameters():\n",
    "    print(weight.data.shape)\n",
    "    i+=[weight.data.shape]\n",
    "print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "a.append(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN_Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
